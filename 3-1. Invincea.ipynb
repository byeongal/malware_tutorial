{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 특징 추출 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "import getopt\n",
    "import pickle\n",
    "import hashlib\n",
    "import multiprocessing as mp\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import cv2\n",
    "import pefile\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn.feature_extraction import FeatureHasher\n",
    "\n",
    "def get_entropy(bytes):\n",
    "    if not bytes:\n",
    "        return 0.0\n",
    "    occurences = Counter(bytes)\n",
    "    entropy = 0\n",
    "    for x in occurences.values():\n",
    "        p_x = float(x) / len(bytes)\n",
    "        entropy -= p_x * math.log(p_x, 2)\n",
    "    return entropy\n",
    "\n",
    "def byte_entropy_histogram(bytez):\n",
    "    output = [0] * 256\n",
    "    for i in range(0, len(bytez), 256):\n",
    "        block = bytez[i:i+1024]\n",
    "        entropy = get_entropy(block)\n",
    "        entropy = int(entropy * 2)\n",
    "        if entropy == 16:\n",
    "            entropy = 15\n",
    "        for j in block:\n",
    "            output[entropy * 16 + j >> 4] += 1\n",
    "    max_value, min_value = max(output), min(output)\n",
    "    if max_value == min_value:\n",
    "        return np.zeros(256, dtype=np.float32)\n",
    "    else:\n",
    "        output = [ (each - min_value) / (max_value - min_value) for each in output ]\n",
    "        return np.array(output)\n",
    "\n",
    "def pe_import(pe):\n",
    "    if hasattr(pe, 'DIRECTORY_ENTRY_IMPORT'):\n",
    "        import_info = []\n",
    "        for entry in pe.DIRECTORY_ENTRY_IMPORT:\n",
    "            dll = entry.dll.decode('ascii')\n",
    "            for imp in entry.imports:\n",
    "                try:\n",
    "                    function = imp.name.decode('ascii')\n",
    "                except:\n",
    "                    function = str(imp.name)\n",
    "                else:\n",
    "                    pass\n",
    "                import_info.append('{}.{}'.format(dll, function))\n",
    "        libraries_hashed = FeatureHasher(256, input_type=\"string\", alternate_sign=False).transform([import_info]).toarray()[0]\n",
    "        max_value, min_value = max(libraries_hashed), min(libraries_hashed)\n",
    "        if max_value == min_value:\n",
    "            return np.zeros(256, dtype=np.float32)\n",
    "        else:\n",
    "            output = [(each - min_value) / (max_value - min_value) for each in libraries_hashed]\n",
    "            return output\n",
    "    else:\n",
    "        return np.zeros(256, dtype=np.float32)\n",
    "\n",
    "def string_2d_histogram(bytez):\n",
    "    output = np.zeros((16, 16), dtype=np.int)\n",
    "    ascii_strings = re.compile(b'[\\x20-\\x7f]{6,}')\n",
    "    ascii_strings = ascii_strings.findall(bytez)\n",
    "    for string in ascii_strings:\n",
    "        y = min(int(np.log(len(string)) / np.log(1.25)), 15)\n",
    "        x = int(hashlib.md5(string).hexdigest(), 16) & 15\n",
    "        output[y][x] += 1\n",
    "    output = output.flatten()\n",
    "    max_value, min_value = max(output), min(output)\n",
    "    if max_value == min_value:\n",
    "        return np.zeros(256, dtype=np.float32)\n",
    "    else:\n",
    "        output = [ (each - min_value) / (max_value - min_value) for each in output ]\n",
    "        return np.array(output)\n",
    "\n",
    "def pe_metadata(pe):\n",
    "    ret = []\n",
    "    def split_to_byte_list(value, size):\n",
    "        ret = []\n",
    "        for i in range(size):\n",
    "            ret.append( (value & 255) / 255.0 )\n",
    "            value >>= 8\n",
    "        return ret\n",
    "\n",
    "    if hasattr(pe, 'FILE_HEADER'):\n",
    "        FILE_HEADER = [\n",
    "            ('Machine', 2),  # The architecture type of the computer.\n",
    "            ('NumberOfSections', 2),  # The number of sections.\n",
    "            ('TimeDateStamp', 4), # The low 32 bits of the time stamp of the image.\n",
    "            ('PointerToSymbolTable', 4),  # The offset of the symbol table, in bytes, or zero if no COFF symbol table exists.\n",
    "            ('NumberOfSymbols', 4),  # The number of symbols in the symbol table.\n",
    "            ('SizeOfOptionalHeader', 2),  # The size of the optional header, in bytes.\n",
    "            ('Characteristics', 2)  # The characteristics of the image.\n",
    "        ]\n",
    "        for field, size in FILE_HEADER:\n",
    "            ret.extend(split_to_byte_list(getattr(pe.FILE_HEADER, field, 0), size))\n",
    "    else:\n",
    "        ret.extend([0.0] * 20)\n",
    "\n",
    "    if hasattr(pe, 'OPTIONAL_HEADER'):\n",
    "        OPTIONAL_HEADER = [\n",
    "            # The state of the image file.\n",
    "            ('Magic', 2),\n",
    "            # The major version number of the linker.\n",
    "            ('MajorLinkerVersion', 1),\n",
    "            # The minor version number of the linker.\n",
    "            ('MinorLinkerVersion', 1),\n",
    "            # The size of the code section, in bytes, or the sum of all such sections if there are multiple code sections.\n",
    "            ('SizeOfCode', 4),\n",
    "            # The size of the initialized data section, in bytes, or the sum of all such sections if there are multiple initialized data sections.\n",
    "            ('SizeOfInitializedData', 4),\n",
    "            # The size of the uninitialized data section, in bytes, or the sum of all such sections if there are multiple uninitialized data sections.\n",
    "            ('SizeOfUninitializedData', 4),\n",
    "            # A pointer to the entry point function, relative to the image base address.\n",
    "            ('AddressOfEntryPoint', 4),\n",
    "            # A pointer to the beginning of the code section, relative to the image base.\n",
    "            ('BaseOfCode', 4),\n",
    "            # A pointer to the beginning of the data section, relative to the image base.\n",
    "            ('BaseOfData', 4),\n",
    "            # The preferred address of the first byte of the image when it is loaded in memory.\n",
    "            ('ImageBase', 4),  # ('ImageBase', 8), PE32+\n",
    "            # The alignment of sections loaded in memory, in bytes.\n",
    "            ('SectionAlignment', 4),\n",
    "            # The alignment of the raw data of sections in the image file, in bytes.\n",
    "            ('FileAlignment', 4),\n",
    "            # The major version number of the required operating system.\n",
    "            ('MajorOperatingSystemVersion', 2),\n",
    "            # The minor version number of the required operating system.\n",
    "            ('MinorOperatingSystemVersion', 2),\n",
    "            # The major version number of the image.\n",
    "            ('MajorImageVersion', 2),\n",
    "            # The minor version number of the image.\n",
    "            ('MinorImageVersion', 2),\n",
    "            # The major version number of the subsystem.\n",
    "            ('MajorSubsystemVersion', 2),\n",
    "            # The minor version number of the subsystem.\n",
    "            ('MinorSubsystemVersion', 2),\n",
    "            # (Win32VersionValue) This member is reserved and must be 0.\n",
    "            ('Reserved1', 4),  # Win32VersionValue\n",
    "            # The size of the image, in bytes, including all headers.\n",
    "            ('SizeOfImage', 4),\n",
    "            # The combined size of the following items, rounded to a multiple of the value specified in the FileAlignment member.\n",
    "            ('SizeOfHeaders', 4),\n",
    "            # The image file checksum.\n",
    "            ('CheckSum', 4),\n",
    "            # The subsystem required to run this image.\n",
    "            ('Subsystem', 2),\n",
    "            # The DLL characteristics of the image.\n",
    "            #('DllCharacteristics', 2),\n",
    "            # The number of bytes to reserve for the stack.\n",
    "            ('SizeOfStackReserve', 4),  # ('SizeOfStackReserve', 8) PE32+\n",
    "            # The number of bytes to commit for the stack.\n",
    "            ('SizeOfStackCommit', 4),  # ('SizeOfStackCommit', 8) PE32+\n",
    "            # The number of bytes to commit for the local heap.\n",
    "            ('SizeOfHeapReserve', 4),  # ('SizeOfHeapReserve', 8) PE32+\n",
    "            # This member is obsolete.\n",
    "            ('SizeOfHeapCommit', 4),  # ('SizeOfHeapCommit', 8) PE32+\n",
    "            # The number of directory entries in the remainder of the optional header.\n",
    "            ('LoaderFlags', 4),\n",
    "            # A pointer to the first IMAGE_DATA_DIRECTORY structure in the data directory.\n",
    "            ('NumberOfRvaAndSizes', 4),\n",
    "        ]\n",
    "        for field, size in OPTIONAL_HEADER:\n",
    "            ret.extend(split_to_byte_list(getattr(pe.OPTIONAL_HEADER, field, 0), size))\n",
    "        dll_characteristics = [0.0] * len(pefile.dll_characteristics)\n",
    "        dll_characteristics_value = getattr(pe.OPTIONAL_HEADER, 'DllCharacteristics', 0)\n",
    "        for i, (constant, value) in enumerate(pefile.dll_characteristics):\n",
    "            if dll_characteristics_value & value == value:\n",
    "                dll_characteristics[i] = 1.0\n",
    "        ret.extend(dll_characteristics)\n",
    "        if hasattr(pe.OPTIONAL_HEADER, 'DATA_DIRECTORY'):\n",
    "            directory_entry_types_dict = {\n",
    "                'IMAGE_DIRECTORY_ENTRY_EXPORT': 0,\n",
    "                'IMAGE_DIRECTORY_ENTRY_IMPORT': 1,\n",
    "                'IMAGE_DIRECTORY_ENTRY_RESOURCE': 2,\n",
    "                'IMAGE_DIRECTORY_ENTRY_EXCEPTION': 3,\n",
    "                'IMAGE_DIRECTORY_ENTRY_SECURITY': 4,\n",
    "                'IMAGE_DIRECTORY_ENTRY_BASERELOC': 5,\n",
    "                'IMAGE_DIRECTORY_ENTRY_DEBUG': 6,\n",
    "                'IMAGE_DIRECTORY_ENTRY_COPYRIGHT': 7,\n",
    "                'IMAGE_DIRECTORY_ENTRY_GLOBALPTR': 8,\n",
    "                'IMAGE_DIRECTORY_ENTRY_TLS': 9,\n",
    "                'IMAGE_DIRECTORY_ENTRY_LOAD_CONFIG': 10,\n",
    "                'IMAGE_DIRECTORY_ENTRY_BOUND_IMPORT': 11,\n",
    "                'IMAGE_DIRECTORY_ENTRY_IAT': 12,\n",
    "                'IMAGE_DIRECTORY_ENTRY_DELAY_IMPORT': 13,\n",
    "                'IMAGE_DIRECTORY_ENTRY_COM_DESCRIPTOR': 14,\n",
    "                'IMAGE_DIRECTORY_ENTRY_RESERVED': 15\n",
    "            }\n",
    "            data_directory = [0.0] * 128\n",
    "            for member in pe.OPTIONAL_HEADER.DATA_DIRECTORY:\n",
    "                idx = directory_entry_types_dict[member.name]\n",
    "                idx = idx * 8\n",
    "                data_directory[idx:idx+4] = split_to_byte_list(member.Size, 4)\n",
    "                data_directory[idx+4:idx + 8] = split_to_byte_list(member.VirtualAddress, 4)\n",
    "            ret.extend(data_directory)\n",
    "        else:\n",
    "            ret.extend([0.0] * 128)\n",
    "    else:\n",
    "        ret.extend([0.0] * 109)\n",
    "        ret.extend([0.0] * 128)\n",
    "    return np.array(ret, dtype=np.float32)\n",
    "\n",
    "def preprocessing_invincea(argv):\n",
    "    # train data binary_file_path, output_file_path, label\n",
    "    if len(argv) == 3:\n",
    "        bin_file_path, output_file_path, label = argv\n",
    "    # inference data binary_file_path, output_file_path\n",
    "    else:\n",
    "        bin_file_path, output_file_path = argv\n",
    "        label = None\n",
    "    with open(bin_file_path, 'rb') as f:\n",
    "        data = f.read()\n",
    "    a = byte_entropy_histogram(data)\n",
    "    c = string_2d_histogram(data)\n",
    "    try:\n",
    "        pe = pefile.PE(data=data)\n",
    "        b = pe_import(pe)\n",
    "        d = pe_metadata(pe)\n",
    "    except:\n",
    "        b = np.zeros(256, dtype=np.float32)\n",
    "        d = np.zeros(257, dtype=np.float32)\n",
    "    vector = np.concatenate([a, b, c, d], axis=0)\n",
    "    with open(output_file_path + '.invin', 'wb') as f:\n",
    "        pickle.dump(vector, f)\n",
    "    if label == None:\n",
    "        return output_file_path + '.invin'\n",
    "    else:\n",
    "        return (output_file_path + '.invin', label)\n",
    "\n",
    "def preprocessing_mcnn(argv):\n",
    "    # train data binary_file_path, output_file_path, label\n",
    "    if len(argv) == 3:\n",
    "        bin_file_path, output_file_path, label = argv\n",
    "    # inference data binary_file_path, output_file_path\n",
    "    else:\n",
    "        bin_file_path, output_file_path = argv\n",
    "        label = None\n",
    "\n",
    "    with open(bin_file_path, 'rb') as f:\n",
    "        data = f.read()\n",
    "        \n",
    "    file_size = len(data)\n",
    "\n",
    "    if file_size < 10 * 1000:\n",
    "        image_width = 32\n",
    "    elif file_size < 30 * 1000:\n",
    "        image_width = 64\n",
    "    elif file_size < 60 * 1000:\n",
    "        image_width = 128\n",
    "    elif file_size < 100 * 1000:\n",
    "        image_width = 256\n",
    "    elif file_size < 200 * 1000:\n",
    "        image_width = 384\n",
    "    elif file_size < 500 * 1000:\n",
    "        image_width = 512\n",
    "    elif file_size < 1000 * 1000:\n",
    "        image_width = 768\n",
    "    else:\n",
    "        image_width = 1024\n",
    "    image_height = file_size // image_width\n",
    "    file_array = np.array(list(data[:image_width * image_height]))\n",
    "    file_img = np.reshape(file_array, (image_height, image_width))\n",
    "    file_img = np.uint8(file_img)\n",
    "    file_img = cv2.resize(file_img, (224, 224), interpolation=cv2.INTER_NEAREST)\n",
    "    cv2.imwrite(output_file_path + '.png', file_img)\n",
    "    if label == None:\n",
    "        return output_file_path + '.png'\n",
    "    else:\n",
    "        return (output_file_path + '.png', label)\n",
    "\n",
    "# PE-Miner DLLs referred(73)\n",
    "def dlls_referred(pe, ret):\n",
    "    __LIST_OF_DLLS = [\n",
    "        'ADVAP132.DLL',  # Advanced Win32 application programming interfaces\n",
    "        'AWFAXP32.DLL',  # Mail API fax transport\n",
    "        'AWFXAB32.DLL',  # Address book\n",
    "        'AWPWD32.DLL',  # Security support\n",
    "        'AWRESX32.DLL',  # Resource Executor\n",
    "        'AWUTIL32.DLL',  # At Work Security Support\n",
    "        'BHNETB.DLL',  # Network monitor SMS client\n",
    "        'BHSUPP.DLL',  # Network monitor SMS client\n",
    "        'CCAPI.DLL',  # Microsoft Network component\n",
    "        'CCEI.DLL',  # Microsoft Network component\n",
    "        'CCPSH.DLL',  # Microsoft Network component\n",
    "        'CCTN20.DLL',  # Microsoft Network component\n",
    "        'CMC.DLL',  # Common messaging calls for Mail API 1.0\n",
    "        'COMCTL32.DLL',  # User Experience Controls Library\n",
    "        'COMDLG32.DLL',  # Common Dialogue Library\n",
    "        'CRTDLL.DLL',  # Microsoft C Runtime Library\n",
    "        'DCIMAN.DLL',  # Display Control Interface Manager\n",
    "        'DCIMAN32.DLL',  # Display Control Interface Manager\n",
    "        'DSKMAINT.DLL',  # Disk Utilities engine\n",
    "        'GDI32.DLL',  # GDI Client DLL\n",
    "        'GROUP.DLL',  # policy support\n",
    "        'HYPERTERM.DLL',  # Terminal DLL\n",
    "        'KERNL32.DLL',  # Windows NT BASE API Client DLL\n",
    "        'LZ32.DLL',  # LZ Expand/Compress API DLL\n",
    "        'MAPI.DLL',  # Mail / Exchange component\n",
    "        'MAPI32.DLL',  # Extended MAPI 1.0 for Windows NT\n",
    "        'MFC30.DLL',  # Shared MFC DLL\n",
    "        'MPR.DLL',  # Multiple Provider Router DLL\n",
    "        'MSPST32.DLL',  # Microsoft Personal Folder/Address Book Service Provider\n",
    "        'MSFS32.DLL',  # MAPI 1.0 Service Providers for Microsoft Mail\n",
    "        'MSNDUI.DLL',  # Microsoft Network component\n",
    "        'MSNET32.DLL',  # Microsoft 32-bit Network API Library\n",
    "        'MSSHRUI.DLL',  # Shell extensions for sharing\n",
    "        'MSVIEWUT.DLL',  # Service data-link libraries for display engines\n",
    "        'NAL.DLL',  # Network monitor SMS client\n",
    "        'NDIS30.DLL',  # Network monitor SMS client\n",
    "        'NETAPI.DLL',  # Network API\n",
    "        'NETAPI32.DLL',  # Net Win32 API DLL\n",
    "        'NETBIOS.DLL',  # NetBIOS API Library\n",
    "        'NETDI.DLL',  # Net Device installer\n",
    "        'NETSETUP.DLL',  # Network server-based setup\n",
    "        'NWAB32.DLL',  # Address book provider\n",
    "        'NWNET32.DLL',  # NetWare client\n",
    "        'NWNP32.DLL',  # NetWare component\n",
    "        'OLEDLG.DLL',  # Microsoft Windows OLE 2.0 User Interface Support\n",
    "        'POWERCFG.DLL',  # Advanced Power Management Control Panel\n",
    "        'RASPI.DLL',  # Automated Software Profile, Analysis, Removal and Signature Information\n",
    "        'RASAPI16.DLL',  # Remote Access Services 16-bit API Library\n",
    "        'RASAPI32.DLL',  # Remote Access 16-bit API Library\n",
    "        'RPCRT4.DLL',  # Remote Procedure Call Runtime\n",
    "        'RPCLTC1.DLL',  # Remote Procedure Call libraries\n",
    "        'RPCTLC3.DLL',  # Remote Procedure Call libraries\n",
    "        'RPCTLC5.DLL',  # Remote Procedure Call libraries\n",
    "        'RPCTLC6.DLL',  # Remote Procedure Call libraries\n",
    "        'RPCTLS3.DLL',  # Remote Procedure Call libraries\n",
    "        'RPCTLS5.DLL',  # Remote Procedure Call libraries\n",
    "        'RPCTLS6.DLL',  # Remote Procedure Call libraries\n",
    "        'RPCNS4.DLL',  # Remote Procedure Call Name Service Client\n",
    "        'RSRC32.DLL',  # Resource Meter\n",
    "        'SAPNSP.DLL',  # Winsock data-link library\n",
    "        'SECUR32.DLL',  # Security Support Provider Interface\n",
    "        'SHELL32.DLL',  # Windows Shell Common DLL\n",
    "        'SLENH.DLL',  # Advanced Power Management options\n",
    "        'SHLWAPI.DLL',  # Library for UNC and URL Paths, Registry Entries and Color Settings\n",
    "        'UMDM32.DLL',  # Universal Modem Driver component\n",
    "        'USER32.DLL',  # USER API Client DLL\n",
    "        'VERSION.DLL',  # Version Checking and File Installation Libraries\n",
    "        'WININET.DLL',  # Internet Extensions for Win32\n",
    "        'WINMM.DLL',  # MCI API DLL\n",
    "        'WINREG.DLL',  # Remote Registry support\n",
    "        'WINSOCK.DLL',  # Socket API for Windows\n",
    "        'WS2.DLL',  # 32.DLL Windows Socket 2.0 32-Bit DLL\n",
    "        'WSOCK32.DLL',  # Windows Socket 32-Bit DLL\n",
    "    ]\n",
    "    for dll in __LIST_OF_DLLS: # 73\n",
    "        ret[dll] = 0\n",
    "    if pe == None:\n",
    "        return\n",
    "    else:\n",
    "        if hasattr(pe, 'DIRECTORY_ENTRY_IMPORT'):\n",
    "            for entry in pe.DIRECTORY_ENTRY_IMPORT:\n",
    "                if isinstance(entry.dll, bytes):\n",
    "                    libname = entry.dll.decode().upper()\n",
    "                else:\n",
    "                    libname = entry.dll.upper()\n",
    "                if libname in ret:\n",
    "                    ret[libname] = 1\n",
    "\n",
    "# PE-Miner COFF file header(7)\n",
    "def coff_file_header(pe, ret):\n",
    "    __FILE_HEADER = [\n",
    "        'Machine',  # The architecture type of the computer.\n",
    "        'NumberOfSections',  # The number of sections.\n",
    "        'TimeDateStamp',  # The low 32 bits of the time stamp of the image.\n",
    "        'PointerToSymbolTable',  # The offset of the symbol table, in bytes, or zero if no COFF symbol table exists.\n",
    "        'NumberOfSymbols',  # The number of symbols in the symbol table.\n",
    "        'SizeOfOptionalHeader',  # The size of the optional header, in bytes.\n",
    "        'Characteristics'  # The characteristics of the image.\n",
    "    ]\n",
    "    for file_header in __FILE_HEADER: # 7\n",
    "        ret['FileHeader.{}'.format(file_header)] = 0\n",
    "    if pe == None:\n",
    "        return\n",
    "    if hasattr(pe, 'FILE_HEADER'):\n",
    "        for member in __FILE_HEADER:\n",
    "            ret['FileHeader.{}'.format(member)] = getattr(pe.FILE_HEADER, member, 0)\n",
    "\n",
    "# PE-Miner Optional header – standard fields(9) + Optional header – Windows specific fields(21)\n",
    "def optional_header(pe, ret):\n",
    "    __OPTIONAL_HEADER = [\n",
    "        # - * - Standard COFF Fields - * -\n",
    "        'Magic',  # The state of the image file.\n",
    "        'MajorLinkerVersion',  # The major version number of the linker.\n",
    "        'MinorLinkerVersion',  # The minor version number of the linker.\n",
    "        'SizeOfCode',\n",
    "        # The size of the code section, in bytes, or the sum of all such sections if there are multiple code sections.\n",
    "        'SizeOfInitializedData',\n",
    "        # The size of the initialized data section, in bytes, or the sum of all such sections if there are multiple initialized data sections.\n",
    "        'SizeOfUninitializedData',\n",
    "        # The size of the uninitialized data section, in bytes, or the sum of all such sections if there are multiple uninitialized data sections.\n",
    "        'AddressOfEntryPoint',  # A pointer to the entry point function, relative to the image base address.\n",
    "        'BaseOfCode',  # A pointer to the beginning of the code section, relative to the image base.\n",
    "        'BaseOfData',  # A pointer to the beginning of the data section, relative to the image base.\n",
    "        # - * - Windows Specific Fields - * -\n",
    "        'ImageBase',  # The preferred address of the first byte of the image when it is loaded in memory.\n",
    "        'SectionAlignment',  # The alignment of sections loaded in memory, in bytes.\n",
    "        'FileAlignment',  # The alignment of the raw data of sections in the image file, in bytes.\n",
    "        'MajorOperatingSystemVersion',  # The major version number of the required operating system.\n",
    "        'MinorOperatingSystemVersion',  # The minor version number of the required operating system.\n",
    "        'MajorImageVersion',  # The major version number of the image.\n",
    "        'MinorImageVersion',  # The minor version number of the image.\n",
    "        'MajorSubsystemVersion',  # The major version number of the subsystem.\n",
    "        'MinorSubsystemVersion',  # The minor version number of the subsystem.\n",
    "        'Reserved1',  # (Win32VersionValue) This member is reserved and must be 0.\n",
    "        'SizeOfImage',  # The size of the image, in bytes, including all headers.\n",
    "        'SizeOfHeaders',\n",
    "        # The combined size of the following items, rounded to a multiple of the value specified in the FileAlignment member.\n",
    "        'CheckSum',  # The image file checksum.\n",
    "        'Subsystem',  # The subsystem required to run this image.\n",
    "        'DllCharacteristics',  # The DLL characteristics of the image.\n",
    "        'SizeOfStackReserve',  # The number of bytes to reserve for the stack.\n",
    "        'SizeOfStackCommit',  # The number of bytes to commit for the stack.\n",
    "        'SizeOfHeapReserve',  # The number of bytes to commit for the local heap.\n",
    "        'SizeOfHeapCommit',  # This member is obsolete.\n",
    "        'LoaderFlags',  # The number of directory entries in the remainder of the optional header.\n",
    "        'NumberOfRvaAndSizes'  # A pointer to the first IMAGE_DATA_DIRECTORY structure in the data directory.\n",
    "    ]\n",
    "    for optional_header in __OPTIONAL_HEADER: # 9 + 21 ( error : -1 )\n",
    "        ret['OptionalHeader.{}'.format(optional_header)] = 0\n",
    "    if pe == None:\n",
    "        return\n",
    "    if hasattr(pe, 'OPTIONAL_HEADER'):\n",
    "        for member in __OPTIONAL_HEADER:\n",
    "            ret['OptionalHeader.{}'.format(member)] = getattr(pe.OPTIONAL_HEADER, member, 0)\n",
    "\n",
    "# PE-Miner Optional header – data directories(30)\n",
    "def optional_header_datadirectory(pe, ret):\n",
    "    __DATA_DIRECTORY = [\n",
    "        'IMAGE_DIRECTORY_ENTRY_EXPORT',\n",
    "        'IMAGE_DIRECTORY_ENTRY_IMPORT',\n",
    "        'IMAGE_DIRECTORY_ENTRY_RESOURCE',\n",
    "        'IMAGE_DIRECTORY_ENTRY_EXCEPTION',\n",
    "        'IMAGE_DIRECTORY_ENTRY_SECURITY',\n",
    "        'IMAGE_DIRECTORY_ENTRY_BASERELOC',\n",
    "        'IMAGE_DIRECTORY_ENTRY_DEBUG',\n",
    "        'IMAGE_DIRECTORY_ENTRY_GLOBALPTR',\n",
    "        'IMAGE_DIRECTORY_ENTRY_TLS',\n",
    "        'IMAGE_DIRECTORY_ENTRY_LOAD_CONFIG',\n",
    "        'IMAGE_DIRECTORY_ENTRY_BOUND_IMPORT',\n",
    "        'IMAGE_DIRECTORY_ENTRY_IAT',\n",
    "        'IMAGE_DIRECTORY_ENTRY_DELAY_IMPORT',\n",
    "        'IMAGE_DIRECTORY_ENTRY_COM_DESCRIPTOR',\n",
    "        'IMAGE_DIRECTORY_ENTRY_RESERVED',\n",
    "    ]\n",
    "    for data_directory in __DATA_DIRECTORY: # 15 * 2\n",
    "        ret['OptionalHeader.DataDirectory.{}.VirtualAddress'.format(data_directory)] = 0\n",
    "        ret['OptionalHeader.DataDirectory.{}.Size'.format(data_directory)] = 0\n",
    "    if pe == None:\n",
    "        return\n",
    "    if hasattr(pe, 'OPTIONAL_HEADER') and hasattr(pe.OPTIONAL_HEADER, 'DATA_DIRECTORY'):\n",
    "        for structure in pe.OPTIONAL_HEADER.DATA_DIRECTORY:\n",
    "            if 'OptionalHeader.DataDirectory.{}.VirtualAddress'.format(structure.name) in ret:\n",
    "                ret['OptionalHeader.DataDirectory.{}.VirtualAddress'.format(\n",
    "                    structure.name)] = structure.VirtualAddress\n",
    "                ret['OptionalHeader.DataDirectory.{}.Size'.format(structure.name)] = structure.Size\n",
    "\n",
    "# PE-Miner .text section – header fields(9) + .data section – header fields(9) + .rsrc section – header fields(9)\n",
    "def section(pe, ret):\n",
    "    __SECTION_HEADER = [\n",
    "        'VirtualSize',\n",
    "        'VirtualAddress',\n",
    "        'SizeOfRawData',\n",
    "        'PointerToRawData',\n",
    "        'PointerToRelocations',\n",
    "        'PointerToLinenumbers',\n",
    "        'NumberOfRelocations',\n",
    "        'NumberOfLinenumbers',\n",
    "        'Characteristics'\n",
    "    ]\n",
    "    for section_name in ['text', 'data', 'rsrc']: # 3 * 9 = 27\n",
    "        for section_header in __SECTION_HEADER:\n",
    "            ret['Section.{}.{}'.format(section_name, section_header)] = 0\n",
    "    if hasattr(pe, 'sections'):\n",
    "        for section in pe.sections:\n",
    "            try:\n",
    "                section_name = str(section.Name, 'utf-8').encode('ascii', errors='ignore').strip().decode(\n",
    "                    'ascii').strip(' \\t\\r\\n\\0')\n",
    "            except:\n",
    "                section_name = str(section.Name, 'ISO-8859-1').encode('ascii', errors='ignore').strip().decode(\n",
    "                    'ascii').strip(' \\t\\r\\n\\0')\n",
    "            if section_name == '':\n",
    "                section_name = '.noname'\n",
    "            if section_name == '.text' or section_name == '.data' or section_name == '.rsrc':\n",
    "                section_name = section_name[1:]\n",
    "                ret['Section.{}.VirtualSize'.format(section_name)] = section.Misc_VirtualSize\n",
    "                ret['Section.{}.VirtualAddress'.format(section_name)] = section.VirtualAddress\n",
    "                ret['Section.{}.SizeOfRawData'.format(section_name)] = section.SizeOfRawData\n",
    "                ret['Section.{}.PointerToRawData'.format(section_name)] = section.PointerToRawData\n",
    "                ret['Section.{}.PointerToRelocations'.format(section_name)] = section.PointerToRelocations\n",
    "                ret['Section.{}.PointerToLinenumbers'.format(section_name)] = section.PointerToLinenumbers\n",
    "                ret['Section.{}.NumberOfRelocations'.format(section_name)] = section.NumberOfRelocations\n",
    "                ret['Section.{}.NumberOfLinenumbers'.format(section_name)] = section.NumberOfLinenumbers\n",
    "                ret['Section.{}.Characteristics'.format(section_name)] = section.Characteristics\n",
    "\n",
    "\n",
    "def resource_directory_table_n_resources(pe, ret):\n",
    "    __RESOURCE_DIRECTORY_TABLE = [\n",
    "        'Characteristics',\n",
    "        'MajorVersion',\n",
    "        'MinorVersion',\n",
    "        'NumberOfIdEntries',\n",
    "        'NumberOfNamedEntries',\n",
    "        'TimeDateStamp'\n",
    "    ]\n",
    "    __RESOURCE_TYPE = [\n",
    "        'RT_CURSOR',\n",
    "        'RT_BITMAP',\n",
    "        'RT_ICON',\n",
    "        'RT_MENU',\n",
    "        'RT_DIALOG',\n",
    "        'RT_STRING',\n",
    "        'RT_FONTDIR',\n",
    "        'RT_FONT',\n",
    "        'RT_ACCELERATOR',\n",
    "        'RT_RCDATA',\n",
    "        'RT_MESSAGETABLE',\n",
    "        'RT_GROUP_CURSOR',\n",
    "        'RT_GROUP_ICON',\n",
    "        'RT_VERSION',\n",
    "        'RT_DLGINCLUDE',\n",
    "    ]\n",
    "    for resource_directory_table in __RESOURCE_DIRECTORY_TABLE:\n",
    "        ret['Resource.{}'.format(resource_directory_table)] = 0\n",
    "    for resource_type in __RESOURCE_TYPE:\n",
    "        ret['Resource.{}'.format(resource_type)] = 0\n",
    "    if pe == None:\n",
    "        return\n",
    "    if hasattr(pe, 'DIRECTORY_ENTRY_RESOURCE'):\n",
    "        ret['Resource.Characteristics'] = pe.DIRECTORY_ENTRY_RESOURCE.struct.Characteristics\n",
    "        ret['Resource.MajorVersion'] = pe.DIRECTORY_ENTRY_RESOURCE.struct.MajorVersion\n",
    "        ret['Resource.MinorVersion'] = pe.DIRECTORY_ENTRY_RESOURCE.struct.MinorVersion\n",
    "        ret['Resource.NumberOfIdEntries'] = pe.DIRECTORY_ENTRY_RESOURCE.struct.NumberOfIdEntries\n",
    "        ret['Resource.NumberOfNamedEntries'] = pe.DIRECTORY_ENTRY_RESOURCE.struct.NumberOfNamedEntries\n",
    "        ret['Resource.TimeDateStamp'] = pe.DIRECTORY_ENTRY_RESOURCE.struct.TimeDateStamp\n",
    "        for resource_type in pe.DIRECTORY_ENTRY_RESOURCE.entries:\n",
    "            name = \"{}\".format(pefile.RESOURCE_TYPE.get(resource_type.struct.Id, 'NONE'))\n",
    "            if 'Resource.{}'.format(name) in ret:\n",
    "                if hasattr(resource_type, 'directory') and hasattr(resource_type.directory, 'entries'):\n",
    "                    ret['Resource.{}'.format(name)] += len(resource_type.directory.entries)\n",
    "\n",
    "def preprocessing_pe_miner(argv):\n",
    "    # train data binary_file_path, output_file_path, label\n",
    "    if len(argv) == 3:\n",
    "        bin_file_path, output_file_path, label = argv\n",
    "    # inference data binary_file_path, output_file_path\n",
    "    else:\n",
    "        bin_file_path, output_file_path = argv\n",
    "        label = None\n",
    "    with open(bin_file_path, 'rb') as f:\n",
    "        file_data = f.read()\n",
    "    try:\n",
    "        pe = pefile.PE(data=file_data)\n",
    "    except:\n",
    "        pe = None\n",
    "    # init\n",
    "    ret = dict()\n",
    "    dlls_referred(pe, ret)\n",
    "    coff_file_header(pe, ret)\n",
    "    section(pe, ret)\n",
    "    resource_directory_table_n_resources(pe, ret)\n",
    "    output_vector = np.array([ value for key, value in sorted(ret.items(), key = lambda x : x[0])])\n",
    "    with open(output_file_path + '.peminer', 'wb') as f:\n",
    "        pickle.dump(output_vector, f)\n",
    "    if label == None:\n",
    "        return output_file_path + '.peminer'\n",
    "    else:\n",
    "        return (output_file_path + '.peminer', label)\n",
    "\n",
    "SUPPORTING_MODEL = {\n",
    "    'PE-Miner' : preprocessing_pe_miner,\n",
    "    'Invincea' : preprocessing_invincea,\n",
    "    'M-CNN' : preprocessing_mcnn,\n",
    "}\n",
    "\n",
    "def preprocess(type_of_model, input_csv_path, output_csv_path, target_dir):\n",
    "    mp.freeze_support()\n",
    "    func = SUPPORTING_MODEL.get(type_of_model)\n",
    "    df = pd.read_csv(input_csv_path)\n",
    "    fn_list = df['file_path']\n",
    "    target_fn_dir = [ os.path.join(target_dir, os.path.basename(fn)) for fn in fn_list ]\n",
    "    if 'label' in df:\n",
    "        label_list = df['label']\n",
    "        with mp.Pool(os.cpu_count()) as pool:\n",
    "            with open(output_csv_path, 'w') as f:\n",
    "                f.write('file_path,label\\n')\n",
    "                for file_path, label in tqdm(pool.imap_unordered(func, zip(fn_list, target_fn_dir, label_list)),\n",
    "                                             total=len(fn_list)):\n",
    "                    f.write('{},{}\\n'.format(file_path, label))\n",
    "        zip(fn_list, target_fn_dir, label_list)\n",
    "    else:\n",
    "        with mp.Pool(os.cpu_count()) as pool:\n",
    "            with open(output_csv_path, 'w') as f:\n",
    "                f.write('file_path,label\\n')\n",
    "                for file_path in tqdm(pool.imap_unordered(func, zip(fn_list, target_fn_dir)),\n",
    "                                             total=len(fn_list)):\n",
    "                    f.write('{}\\n'.format(file_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 샘플 파일로 부터 특징 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./samples/c0306554fda888e1006cf60b31dddd8c.vir', 'rb') as f:\n",
    "    data = f.read()\n",
    "a = byte_entropy_histogram(data)\n",
    "c = string_2d_histogram(data)\n",
    "try:\n",
    "    pe = pefile.PE(data = data)\n",
    "    b = pe_import(pe)\n",
    "    d = pe_metadata(pe)\n",
    "except:\n",
    "    b = np.zeros(256, dtype=np.float64)\n",
    "    d = np.zeros(257, dtype=np.float64)\n",
    "vector = np.concatenate([a, b, c, d], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "최대값 : 1.0\n",
      "최소값 : 0.0\n",
      "형태 : (1025,)\n"
     ]
    }
   ],
   "source": [
    "print('최대값 :', max(vector))\n",
    "print('최소값 :', min(vector))\n",
    "print('형태 :', vector.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KISA 2018 데이터로 부터 특징 벡터 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_df = pd.read_csv('train.csv', header = None)\n",
    "train_md5_list, train_labels = train_df[0], train_df[1]\n",
    "valid_df = pd.read_csv('valid.csv', header = None)\n",
    "valid_md5_list, valid_labels = valid_df[0], valid_df[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "개수 : 20000\n",
      "악성 : 14000\n",
      "정상 : 6000\n"
     ]
    }
   ],
   "source": [
    "print('개수 :', len(train_md5_list))\n",
    "print('악성 :', sum(train_labels))\n",
    "print('정상 :', len(train_md5_list) - sum(train_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e5a12a7d6ef486db267efeb526e0ccc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=20000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "for md5 in tqdm(train_md5_list):\n",
    "    if os.path.exists('./invincea/{}.pkl'.format(md5)):\n",
    "        continue\n",
    "    with open('./kisa_2018/{}.vir'.format(md5), 'rb') as f:\n",
    "        data = f.read()\n",
    "    a = byte_entropy_histogram(data)\n",
    "    c = string_2d_histogram(data)\n",
    "    try:\n",
    "        pe = pefile.PE(data = data)\n",
    "        b = pe_import(pe)\n",
    "        d = pe_metadata(pe)\n",
    "    except:\n",
    "        b = np.zeros(256, dtype=np.float64)\n",
    "        d = np.zeros(257, dtype=np.float64)\n",
    "    vector = np.concatenate([a, b, c, d], axis=0)\n",
    "    with open('./invincea/{}.pkl'.format(md5), 'wb') as f:\n",
    "        pickle.dump(vector, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd8fa5b220f44751b9304135304ae0a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=20000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for md5 in tqdm(valid_md5_list):\n",
    "    if os.path.exists('./invincea/{}.pkl'.format(md5)):\n",
    "        continue\n",
    "    with open('./kisa_2018/{}.vir'.format(md5), 'rb') as f:\n",
    "        data = f.read()\n",
    "    a = byte_entropy_histogram(data)\n",
    "    c = string_2d_histogram(data)\n",
    "    try:\n",
    "        pe = pefile.PE(data = data)\n",
    "        b = pe_import(pe)\n",
    "        d = pe_metadata(pe)\n",
    "    except:\n",
    "        b = np.zeros(256, dtype=np.float64)\n",
    "        d = np.zeros(257, dtype=np.float64)\n",
    "    vector = np.concatenate([a, b, c, d], axis=0)\n",
    "    with open('./invincea/{}.pkl'.format(md5), 'wb') as f:\n",
    "        pickle.dump(vector, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Dense, Flatten, Conv2D, MaxPooling2D\n",
    "from tensorflow.keras import Model\n",
    "\n",
    "def Invincea():\n",
    "    input_layer = Input((1025,))\n",
    "    h1 = Dense(1025, activation='relu')(input_layer)\n",
    "    h2 = Dense(1025, activation='relu')(h1)\n",
    "    output_layer = Dense(1, activation='sigmoid')(h2)\n",
    "    return Model(input_layer, output_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Invincea()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 1025)]            0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1025)              1051650   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1025)              1051650   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 1026      \n",
      "=================================================================\n",
      "Total params: 2,104,326\n",
      "Trainable params: 2,104,326\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from tensorflow.keras.utils import Sequence\n",
    "\n",
    "class DataSequence(Sequence):\n",
    "    def __init__(self, x_set, y_set, batch_size, shuffle=True):\n",
    "        self.x = x_set\n",
    "        self.y = y_set\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        self.indexes = np.arange(len(self.x))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, md5_list):\n",
    "        vector_array = []\n",
    "        for md5 in md5_list:\n",
    "            with open('./invincea/{}.pkl'.format(md5), 'rb') as f:\n",
    "                vector_array.append(pickle.load(f))\n",
    "        return np.array(vector_array)\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.x) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        indexes = self.indexes[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "        batch_x = self.x[indexes]\n",
    "        batch_y = self.y[indexes]\n",
    "        return self.__data_generation(batch_x), np.array(batch_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = DataSequence(train_md5_list, train_labels, 128)\n",
    "valid_generator = DataSequence(valid_md5_list, valid_labels, 128, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/32\n",
      "157/157 [==============================] - 26s 166ms/step - loss: 0.2622 - acc: 0.8921 - val_loss: 0.2403 - val_acc: 0.9075\n",
      "Epoch 2/32\n",
      "157/157 [==============================] - 16s 101ms/step - loss: 0.1636 - acc: 0.9327 - val_loss: 0.2305 - val_acc: 0.9120\n",
      "Epoch 3/32\n",
      "157/157 [==============================] - 16s 101ms/step - loss: 0.1294 - acc: 0.9485 - val_loss: 0.2861 - val_acc: 0.8970\n",
      "Epoch 4/32\n",
      "157/157 [==============================] - 16s 101ms/step - loss: 0.1122 - acc: 0.9550 - val_loss: 0.2663 - val_acc: 0.9065\n",
      "Epoch 5/32\n",
      "157/157 [==============================] - 16s 101ms/step - loss: 0.1089 - acc: 0.9548 - val_loss: 0.3009 - val_acc: 0.9085\n",
      "Epoch 6/32\n",
      "157/157 [==============================] - 16s 101ms/step - loss: 0.0897 - acc: 0.9643 - val_loss: 0.2768 - val_acc: 0.9168\n",
      "Epoch 7/32\n",
      "157/157 [==============================] - 16s 101ms/step - loss: 0.0835 - acc: 0.9656 - val_loss: 0.3322 - val_acc: 0.8988\n",
      "Epoch 8/32\n",
      "157/157 [==============================] - 16s 101ms/step - loss: 0.0715 - acc: 0.9697 - val_loss: 0.3593 - val_acc: 0.9054\n",
      "Epoch 9/32\n",
      "157/157 [==============================] - 16s 101ms/step - loss: 0.0695 - acc: 0.9714 - val_loss: 0.4456 - val_acc: 0.9021\n",
      "Epoch 10/32\n",
      "157/157 [==============================] - 16s 101ms/step - loss: 0.0645 - acc: 0.9737 - val_loss: 0.5518 - val_acc: 0.8914\n",
      "Epoch 11/32\n",
      "157/157 [==============================] - 16s 101ms/step - loss: 0.0713 - acc: 0.9700 - val_loss: 0.4160 - val_acc: 0.9028\n",
      "Epoch 12/32\n",
      "157/157 [==============================] - 16s 102ms/step - loss: 0.0626 - acc: 0.9743 - val_loss: 0.5262 - val_acc: 0.8975\n",
      "Epoch 13/32\n",
      "157/157 [==============================] - 16s 101ms/step - loss: 0.0576 - acc: 0.9766 - val_loss: 0.5724 - val_acc: 0.8909\n",
      "Epoch 14/32\n",
      "157/157 [==============================] - 16s 103ms/step - loss: 0.0538 - acc: 0.9779 - val_loss: 0.5868 - val_acc: 0.9029\n",
      "Epoch 15/32\n",
      "157/157 [==============================] - 17s 108ms/step - loss: 0.0536 - acc: 0.9790 - val_loss: 0.6182 - val_acc: 0.9008\n",
      "Epoch 16/32\n",
      "157/157 [==============================] - 17s 110ms/step - loss: 0.0525 - acc: 0.9777 - val_loss: 0.6324 - val_acc: 0.8977\n",
      "Epoch 17/32\n",
      "157/157 [==============================] - 17s 107ms/step - loss: 0.0546 - acc: 0.9779 - val_loss: 0.6718 - val_acc: 0.8953\n",
      "Epoch 18/32\n",
      "157/157 [==============================] - 16s 101ms/step - loss: 0.0470 - acc: 0.9806 - val_loss: 0.6450 - val_acc: 0.8946\n",
      "Epoch 19/32\n",
      "157/157 [==============================] - 16s 102ms/step - loss: 0.0501 - acc: 0.9805 - val_loss: 0.8310 - val_acc: 0.8886\n",
      "Epoch 20/32\n",
      "157/157 [==============================] - 16s 101ms/step - loss: 0.0454 - acc: 0.9819 - val_loss: 0.6700 - val_acc: 0.9002\n",
      "Epoch 21/32\n",
      "157/157 [==============================] - 16s 102ms/step - loss: 0.0422 - acc: 0.9818 - val_loss: 0.7533 - val_acc: 0.8883\n",
      "Epoch 22/32\n",
      "157/157 [==============================] - 16s 102ms/step - loss: 0.0481 - acc: 0.9800 - val_loss: 0.6372 - val_acc: 0.8982\n",
      "Epoch 23/32\n",
      "157/157 [==============================] - 16s 102ms/step - loss: 0.0467 - acc: 0.9812 - val_loss: 0.9477 - val_acc: 0.8862\n",
      "Epoch 24/32\n",
      "157/157 [==============================] - 16s 102ms/step - loss: 0.0432 - acc: 0.9822 - val_loss: 0.6995 - val_acc: 0.8997\n",
      "Epoch 25/32\n",
      "157/157 [==============================] - 16s 101ms/step - loss: 0.0473 - acc: 0.9815 - val_loss: 0.7411 - val_acc: 0.8911\n",
      "Epoch 26/32\n",
      "157/157 [==============================] - 16s 102ms/step - loss: 0.0379 - acc: 0.9837 - val_loss: 0.7751 - val_acc: 0.9001\n",
      "Epoch 27/32\n",
      "157/157 [==============================] - 16s 102ms/step - loss: 0.0444 - acc: 0.9827 - val_loss: 0.6600 - val_acc: 0.8996\n",
      "Epoch 28/32\n",
      "157/157 [==============================] - 16s 102ms/step - loss: 0.0411 - acc: 0.9832 - val_loss: 0.6097 - val_acc: 0.9047\n",
      "Epoch 29/32\n",
      "157/157 [==============================] - 16s 102ms/step - loss: 0.0416 - acc: 0.9837 - val_loss: 0.7342 - val_acc: 0.8976\n",
      "Epoch 30/32\n",
      "157/157 [==============================] - 16s 102ms/step - loss: 0.0333 - acc: 0.9862 - val_loss: 0.8696 - val_acc: 0.8977\n",
      "Epoch 31/32\n",
      "157/157 [==============================] - 16s 102ms/step - loss: 0.0412 - acc: 0.9821 - val_loss: 0.6891 - val_acc: 0.9044\n",
      "Epoch 32/32\n",
      "157/157 [==============================] - 16s 102ms/step - loss: 0.0370 - acc: 0.9837 - val_loss: 0.8087 - val_acc: 0.8985\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs = 32,\n",
    "    validation_data = valid_generator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('invincea.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('invincea.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/157 [==============================] - 8s 51ms/step - loss: 0.8087 - acc: 0.8985\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.8086861371994019, 0.8985499739646912]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(valid_generator)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
