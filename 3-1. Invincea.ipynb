{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 특징 추출 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import math\n",
    "import pickle\n",
    "\n",
    "from collections import Counter\n",
    "import pefile\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn.feature_extraction import FeatureHasher\n",
    "\n",
    "def get_entropy(bytes):\n",
    "    if not bytes:\n",
    "        return 0.0\n",
    "    occurences = Counter(bytes)\n",
    "    entropy = 0\n",
    "    for x in occurences.values():\n",
    "        p_x = float(x) / len(bytes)\n",
    "        entropy -= p_x * math.log(p_x, 2)\n",
    "    return entropy\n",
    "\n",
    "def byte_entropy_histogram(bytez):\n",
    "    output = [0] * 256\n",
    "    for i in range(0, len(bytez), 256):\n",
    "        block = bytez[i:i+1024]\n",
    "        entropy = get_entropy(block)\n",
    "        entropy = int(entropy * 2)\n",
    "        if entropy == 16:\n",
    "            entropy = 15\n",
    "        for j in block:\n",
    "            output[entropy * 16 + j >> 4] += 1\n",
    "    max_value, min_value = max(output), min(output)\n",
    "    if max_value == min_value:\n",
    "        return np.zeros(256, dtype=np.float32)\n",
    "    else:\n",
    "        output = [ (each - min_value) / (max_value - min_value) for each in output ]\n",
    "        return np.array(output)\n",
    "\n",
    "def pe_import(pe):\n",
    "    if hasattr(pe, 'DIRECTORY_ENTRY_IMPORT'):\n",
    "        import_info = []\n",
    "        for entry in pe.DIRECTORY_ENTRY_IMPORT:\n",
    "            dll = entry.dll.decode('ascii')\n",
    "            for imp in entry.imports:\n",
    "                try:\n",
    "                    function = imp.name.decode('ascii')\n",
    "                except:\n",
    "                    function = str(imp.name)\n",
    "                else:\n",
    "                    pass\n",
    "                import_info.append('{}.{}'.format(dll, function))\n",
    "        libraries_hashed = FeatureHasher(256, input_type=\"string\", alternate_sign=False).transform([import_info]).toarray()[0]\n",
    "        max_value, min_value = max(libraries_hashed), min(libraries_hashed)\n",
    "        if max_value == min_value:\n",
    "            return np.zeros(256, dtype=np.float32)\n",
    "        else:\n",
    "            output = [(each - min_value) / (max_value - min_value) for each in libraries_hashed]\n",
    "            return output\n",
    "    else:\n",
    "        return np.zeros(256, dtype=np.float32)\n",
    "\n",
    "def string_2d_histogram(bytez):\n",
    "    output = np.zeros((16, 16), dtype=np.int)\n",
    "    ascii_strings = re.compile(b'[\\x20-\\x7f]{6,}')\n",
    "    ascii_strings = ascii_strings.findall(bytez)\n",
    "    for string in ascii_strings:\n",
    "        y = min(int(np.log(len(string)) / np.log(1.25)), 15)\n",
    "        x = int(hashlib.md5(string).hexdigest(), 16) & 15\n",
    "        output[y][x] += 1\n",
    "    output = output.flatten()\n",
    "    max_value, min_value = max(output), min(output)\n",
    "    if max_value == min_value:\n",
    "        return np.zeros(256, dtype=np.float32)\n",
    "    else:\n",
    "        output = [ (each - min_value) / (max_value - min_value) for each in output ]\n",
    "        return np.array(output)\n",
    "\n",
    "def pe_metadata(pe):\n",
    "    ret = []\n",
    "    def split_to_byte_list(value, size):\n",
    "        ret = []\n",
    "        for i in range(size):\n",
    "            ret.append( (value & 255) / 255.0 )\n",
    "            value >>= 8\n",
    "        return ret\n",
    "\n",
    "    if hasattr(pe, 'FILE_HEADER'):\n",
    "        FILE_HEADER = [\n",
    "            ('Machine', 2),  # The architecture type of the computer.\n",
    "            ('NumberOfSections', 2),  # The number of sections.\n",
    "            ('TimeDateStamp', 4), # The low 32 bits of the time stamp of the image.\n",
    "            ('PointerToSymbolTable', 4),  # The offset of the symbol table, in bytes, or zero if no COFF symbol table exists.\n",
    "            ('NumberOfSymbols', 4),  # The number of symbols in the symbol table.\n",
    "            ('SizeOfOptionalHeader', 2),  # The size of the optional header, in bytes.\n",
    "            ('Characteristics', 2)  # The characteristics of the image.\n",
    "        ]\n",
    "        for field, size in FILE_HEADER:\n",
    "            ret.extend(split_to_byte_list(getattr(pe.FILE_HEADER, field, 0), size))\n",
    "    else:\n",
    "        ret.extend([0.0] * 20)\n",
    "\n",
    "    if hasattr(pe, 'OPTIONAL_HEADER'):\n",
    "        OPTIONAL_HEADER = [\n",
    "            # The state of the image file.\n",
    "            ('Magic', 2),\n",
    "            # The major version number of the linker.\n",
    "            ('MajorLinkerVersion', 1),\n",
    "            # The minor version number of the linker.\n",
    "            ('MinorLinkerVersion', 1),\n",
    "            # The size of the code section, in bytes, or the sum of all such sections if there are multiple code sections.\n",
    "            ('SizeOfCode', 4),\n",
    "            # The size of the initialized data section, in bytes, or the sum of all such sections if there are multiple initialized data sections.\n",
    "            ('SizeOfInitializedData', 4),\n",
    "            # The size of the uninitialized data section, in bytes, or the sum of all such sections if there are multiple uninitialized data sections.\n",
    "            ('SizeOfUninitializedData', 4),\n",
    "            # A pointer to the entry point function, relative to the image base address.\n",
    "            ('AddressOfEntryPoint', 4),\n",
    "            # A pointer to the beginning of the code section, relative to the image base.\n",
    "            ('BaseOfCode', 4),\n",
    "            # A pointer to the beginning of the data section, relative to the image base.\n",
    "            ('BaseOfData', 4),\n",
    "            # The preferred address of the first byte of the image when it is loaded in memory.\n",
    "            ('ImageBase', 4),  # ('ImageBase', 8), PE32+\n",
    "            # The alignment of sections loaded in memory, in bytes.\n",
    "            ('SectionAlignment', 4),\n",
    "            # The alignment of the raw data of sections in the image file, in bytes.\n",
    "            ('FileAlignment', 4),\n",
    "            # The major version number of the required operating system.\n",
    "            ('MajorOperatingSystemVersion', 2),\n",
    "            # The minor version number of the required operating system.\n",
    "            ('MinorOperatingSystemVersion', 2),\n",
    "            # The major version number of the image.\n",
    "            ('MajorImageVersion', 2),\n",
    "            # The minor version number of the image.\n",
    "            ('MinorImageVersion', 2),\n",
    "            # The major version number of the subsystem.\n",
    "            ('MajorSubsystemVersion', 2),\n",
    "            # The minor version number of the subsystem.\n",
    "            ('MinorSubsystemVersion', 2),\n",
    "            # (Win32VersionValue) This member is reserved and must be 0.\n",
    "            ('Reserved1', 4),  # Win32VersionValue\n",
    "            # The size of the image, in bytes, including all headers.\n",
    "            ('SizeOfImage', 4),\n",
    "            # The combined size of the following items, rounded to a multiple of the value specified in the FileAlignment member.\n",
    "            ('SizeOfHeaders', 4),\n",
    "            # The image file checksum.\n",
    "            ('CheckSum', 4),\n",
    "            # The subsystem required to run this image.\n",
    "            ('Subsystem', 2),\n",
    "            # The DLL characteristics of the image.\n",
    "            #('DllCharacteristics', 2),\n",
    "            # The number of bytes to reserve for the stack.\n",
    "            ('SizeOfStackReserve', 4),  # ('SizeOfStackReserve', 8) PE32+\n",
    "            # The number of bytes to commit for the stack.\n",
    "            ('SizeOfStackCommit', 4),  # ('SizeOfStackCommit', 8) PE32+\n",
    "            # The number of bytes to commit for the local heap.\n",
    "            ('SizeOfHeapReserve', 4),  # ('SizeOfHeapReserve', 8) PE32+\n",
    "            # This member is obsolete.\n",
    "            ('SizeOfHeapCommit', 4),  # ('SizeOfHeapCommit', 8) PE32+\n",
    "            # The number of directory entries in the remainder of the optional header.\n",
    "            ('LoaderFlags', 4),\n",
    "            # A pointer to the first IMAGE_DATA_DIRECTORY structure in the data directory.\n",
    "            ('NumberOfRvaAndSizes', 4),\n",
    "        ]\n",
    "        for field, size in OPTIONAL_HEADER:\n",
    "            ret.extend(split_to_byte_list(getattr(pe.OPTIONAL_HEADER, field, 0), size))\n",
    "        dll_characteristics = [0.0] * len(pefile.dll_characteristics)\n",
    "        dll_characteristics_value = getattr(pe.OPTIONAL_HEADER, 'DllCharacteristics', 0)\n",
    "        for i, (constant, value) in enumerate(pefile.dll_characteristics):\n",
    "            if dll_characteristics_value & value == value:\n",
    "                dll_characteristics[i] = 1.0\n",
    "        ret.extend(dll_characteristics)\n",
    "        if hasattr(pe.OPTIONAL_HEADER, 'DATA_DIRECTORY'):\n",
    "            directory_entry_types_dict = {\n",
    "                'IMAGE_DIRECTORY_ENTRY_EXPORT': 0,\n",
    "                'IMAGE_DIRECTORY_ENTRY_IMPORT': 1,\n",
    "                'IMAGE_DIRECTORY_ENTRY_RESOURCE': 2,\n",
    "                'IMAGE_DIRECTORY_ENTRY_EXCEPTION': 3,\n",
    "                'IMAGE_DIRECTORY_ENTRY_SECURITY': 4,\n",
    "                'IMAGE_DIRECTORY_ENTRY_BASERELOC': 5,\n",
    "                'IMAGE_DIRECTORY_ENTRY_DEBUG': 6,\n",
    "                'IMAGE_DIRECTORY_ENTRY_COPYRIGHT': 7,\n",
    "                'IMAGE_DIRECTORY_ENTRY_GLOBALPTR': 8,\n",
    "                'IMAGE_DIRECTORY_ENTRY_TLS': 9,\n",
    "                'IMAGE_DIRECTORY_ENTRY_LOAD_CONFIG': 10,\n",
    "                'IMAGE_DIRECTORY_ENTRY_BOUND_IMPORT': 11,\n",
    "                'IMAGE_DIRECTORY_ENTRY_IAT': 12,\n",
    "                'IMAGE_DIRECTORY_ENTRY_DELAY_IMPORT': 13,\n",
    "                'IMAGE_DIRECTORY_ENTRY_COM_DESCRIPTOR': 14,\n",
    "                'IMAGE_DIRECTORY_ENTRY_RESERVED': 15\n",
    "            }\n",
    "            data_directory = [0.0] * 128\n",
    "            for member in pe.OPTIONAL_HEADER.DATA_DIRECTORY:\n",
    "                idx = directory_entry_types_dict[member.name]\n",
    "                idx = idx * 8\n",
    "                data_directory[idx:idx+4] = split_to_byte_list(member.Size, 4)\n",
    "                data_directory[idx+4:idx + 8] = split_to_byte_list(member.VirtualAddress, 4)\n",
    "            ret.extend(data_directory)\n",
    "        else:\n",
    "            ret.extend([0.0] * 128)\n",
    "    else:\n",
    "        ret.extend([0.0] * 109)\n",
    "        ret.extend([0.0] * 128)\n",
    "    return np.array(ret, dtype=np.float32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 샘플 파일로 부터 특징 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./samples/c0306554fda888e1006cf60b31dddd8c.vir', 'rb') as f:\n",
    "    data = f.read()\n",
    "a = byte_entropy_histogram(data)\n",
    "c = string_2d_histogram(data)\n",
    "try:\n",
    "    pe = pefile.PE(data = data)\n",
    "    b = pe_import(pe)\n",
    "    d = pe_metadata(pe)\n",
    "except:\n",
    "    b = np.zeros(256, dtype=np.float64)\n",
    "    d = np.zeros(257, dtype=np.float64)\n",
    "vector = np.concatenate([a, b, c, d], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "최대값 : 1.0\n",
      "최소값 : 0.0\n",
      "형태 : (1025,)\n"
     ]
    }
   ],
   "source": [
    "print('최대값 :', max(vector))\n",
    "print('최소값 :', min(vector))\n",
    "print('형태 :', vector.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KISA 2018 데이터로 부터 특징 벡터 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_df = pd.read_csv('train.csv', header = None)\n",
    "train_md5_list, train_labels = train_df[0], train_df[1]\n",
    "valid_df = pd.read_csv('valid.csv', header = None)\n",
    "valid_md5_list, valid_labels = valid_df[0], valid_df[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "개수 : 20000\n",
      "악성 : 14000\n",
      "정상 : 6000\n"
     ]
    }
   ],
   "source": [
    "print('개수 :', len(train_md5_list))\n",
    "print('악성 :', sum(train_labels))\n",
    "print('정상 :', len(train_md5_list) - sum(train_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e5a12a7d6ef486db267efeb526e0ccc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=20000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "for md5 in tqdm(train_md5_list):\n",
    "    if os.path.exists('./invincea/{}.pkl'.format(md5)):\n",
    "        continue\n",
    "    with open('./kisa_2018/{}.vir'.format(md5), 'rb') as f:\n",
    "        data = f.read()\n",
    "    a = byte_entropy_histogram(data)\n",
    "    c = string_2d_histogram(data)\n",
    "    try:\n",
    "        pe = pefile.PE(data = data)\n",
    "        b = pe_import(pe)\n",
    "        d = pe_metadata(pe)\n",
    "    except:\n",
    "        b = np.zeros(256, dtype=np.float64)\n",
    "        d = np.zeros(257, dtype=np.float64)\n",
    "    vector = np.concatenate([a, b, c, d], axis=0)\n",
    "    with open('./invincea/{}.pkl'.format(md5), 'wb') as f:\n",
    "        pickle.dump(vector, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd8fa5b220f44751b9304135304ae0a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=20000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for md5 in tqdm(valid_md5_list):\n",
    "    if os.path.exists('./invincea/{}.pkl'.format(md5)):\n",
    "        continue\n",
    "    with open('./kisa_2018/{}.vir'.format(md5), 'rb') as f:\n",
    "        data = f.read()\n",
    "    a = byte_entropy_histogram(data)\n",
    "    c = string_2d_histogram(data)\n",
    "    try:\n",
    "        pe = pefile.PE(data = data)\n",
    "        b = pe_import(pe)\n",
    "        d = pe_metadata(pe)\n",
    "    except:\n",
    "        b = np.zeros(256, dtype=np.float64)\n",
    "        d = np.zeros(257, dtype=np.float64)\n",
    "    vector = np.concatenate([a, b, c, d], axis=0)\n",
    "    with open('./invincea/{}.pkl'.format(md5), 'wb') as f:\n",
    "        pickle.dump(vector, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Dense, Flatten, Conv2D, MaxPooling2D\n",
    "from tensorflow.keras import Model\n",
    "\n",
    "def Invincea():\n",
    "    input_layer = Input((1025,))\n",
    "    h1 = Dense(1025, activation='relu')(input_layer)\n",
    "    h2 = Dense(1025, activation='relu')(h1)\n",
    "    output_layer = Dense(1, activation='sigmoid')(h2)\n",
    "    return Model(input_layer, output_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Invincea()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 1025)]            0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1025)              1051650   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1025)              1051650   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 1026      \n",
      "=================================================================\n",
      "Total params: 2,104,326\n",
      "Trainable params: 2,104,326\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from tensorflow.keras.utils import Sequence\n",
    "\n",
    "class DataSequence(Sequence):\n",
    "    def __init__(self, x_set, y_set, batch_size, shuffle=True):\n",
    "        self.x = x_set\n",
    "        self.y = y_set\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        self.indexes = np.arange(len(self.x))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, md5_list):\n",
    "        vector_array = []\n",
    "        for md5 in md5_list:\n",
    "            with open('./invincea/{}.pkl'.format(md5), 'rb') as f:\n",
    "                vector_array.append(pickle.load(f))\n",
    "        return np.array(vector_array)\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.x) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        indexes = self.indexes[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "        batch_x = self.x[indexes]\n",
    "        batch_y = self.y[indexes]\n",
    "        return self.__data_generation(batch_x), np.array(batch_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = DataSequence(train_md5_list, train_labels, 128)\n",
    "valid_generator = DataSequence(valid_md5_list, valid_labels, 128, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/32\n",
      "157/157 [==============================] - 26s 166ms/step - loss: 0.2622 - acc: 0.8921 - val_loss: 0.2403 - val_acc: 0.9075\n",
      "Epoch 2/32\n",
      "157/157 [==============================] - 16s 101ms/step - loss: 0.1636 - acc: 0.9327 - val_loss: 0.2305 - val_acc: 0.9120\n",
      "Epoch 3/32\n",
      "157/157 [==============================] - 16s 101ms/step - loss: 0.1294 - acc: 0.9485 - val_loss: 0.2861 - val_acc: 0.8970\n",
      "Epoch 4/32\n",
      "157/157 [==============================] - 16s 101ms/step - loss: 0.1122 - acc: 0.9550 - val_loss: 0.2663 - val_acc: 0.9065\n",
      "Epoch 5/32\n",
      "157/157 [==============================] - 16s 101ms/step - loss: 0.1089 - acc: 0.9548 - val_loss: 0.3009 - val_acc: 0.9085\n",
      "Epoch 6/32\n",
      "157/157 [==============================] - 16s 101ms/step - loss: 0.0897 - acc: 0.9643 - val_loss: 0.2768 - val_acc: 0.9168\n",
      "Epoch 7/32\n",
      "157/157 [==============================] - 16s 101ms/step - loss: 0.0835 - acc: 0.9656 - val_loss: 0.3322 - val_acc: 0.8988\n",
      "Epoch 8/32\n",
      "157/157 [==============================] - 16s 101ms/step - loss: 0.0715 - acc: 0.9697 - val_loss: 0.3593 - val_acc: 0.9054\n",
      "Epoch 9/32\n",
      "157/157 [==============================] - 16s 101ms/step - loss: 0.0695 - acc: 0.9714 - val_loss: 0.4456 - val_acc: 0.9021\n",
      "Epoch 10/32\n",
      "157/157 [==============================] - 16s 101ms/step - loss: 0.0645 - acc: 0.9737 - val_loss: 0.5518 - val_acc: 0.8914\n",
      "Epoch 11/32\n",
      "157/157 [==============================] - 16s 101ms/step - loss: 0.0713 - acc: 0.9700 - val_loss: 0.4160 - val_acc: 0.9028\n",
      "Epoch 12/32\n",
      "157/157 [==============================] - 16s 102ms/step - loss: 0.0626 - acc: 0.9743 - val_loss: 0.5262 - val_acc: 0.8975\n",
      "Epoch 13/32\n",
      "157/157 [==============================] - 16s 101ms/step - loss: 0.0576 - acc: 0.9766 - val_loss: 0.5724 - val_acc: 0.8909\n",
      "Epoch 14/32\n",
      "157/157 [==============================] - 16s 103ms/step - loss: 0.0538 - acc: 0.9779 - val_loss: 0.5868 - val_acc: 0.9029\n",
      "Epoch 15/32\n",
      "157/157 [==============================] - 17s 108ms/step - loss: 0.0536 - acc: 0.9790 - val_loss: 0.6182 - val_acc: 0.9008\n",
      "Epoch 16/32\n",
      "157/157 [==============================] - 17s 110ms/step - loss: 0.0525 - acc: 0.9777 - val_loss: 0.6324 - val_acc: 0.8977\n",
      "Epoch 17/32\n",
      "157/157 [==============================] - 17s 107ms/step - loss: 0.0546 - acc: 0.9779 - val_loss: 0.6718 - val_acc: 0.8953\n",
      "Epoch 18/32\n",
      "157/157 [==============================] - 16s 101ms/step - loss: 0.0470 - acc: 0.9806 - val_loss: 0.6450 - val_acc: 0.8946\n",
      "Epoch 19/32\n",
      "157/157 [==============================] - 16s 102ms/step - loss: 0.0501 - acc: 0.9805 - val_loss: 0.8310 - val_acc: 0.8886\n",
      "Epoch 20/32\n",
      "157/157 [==============================] - 16s 101ms/step - loss: 0.0454 - acc: 0.9819 - val_loss: 0.6700 - val_acc: 0.9002\n",
      "Epoch 21/32\n",
      "157/157 [==============================] - 16s 102ms/step - loss: 0.0422 - acc: 0.9818 - val_loss: 0.7533 - val_acc: 0.8883\n",
      "Epoch 22/32\n",
      "157/157 [==============================] - 16s 102ms/step - loss: 0.0481 - acc: 0.9800 - val_loss: 0.6372 - val_acc: 0.8982\n",
      "Epoch 23/32\n",
      "157/157 [==============================] - 16s 102ms/step - loss: 0.0467 - acc: 0.9812 - val_loss: 0.9477 - val_acc: 0.8862\n",
      "Epoch 24/32\n",
      "157/157 [==============================] - 16s 102ms/step - loss: 0.0432 - acc: 0.9822 - val_loss: 0.6995 - val_acc: 0.8997\n",
      "Epoch 25/32\n",
      "157/157 [==============================] - 16s 101ms/step - loss: 0.0473 - acc: 0.9815 - val_loss: 0.7411 - val_acc: 0.8911\n",
      "Epoch 26/32\n",
      "157/157 [==============================] - 16s 102ms/step - loss: 0.0379 - acc: 0.9837 - val_loss: 0.7751 - val_acc: 0.9001\n",
      "Epoch 27/32\n",
      "157/157 [==============================] - 16s 102ms/step - loss: 0.0444 - acc: 0.9827 - val_loss: 0.6600 - val_acc: 0.8996\n",
      "Epoch 28/32\n",
      "157/157 [==============================] - 16s 102ms/step - loss: 0.0411 - acc: 0.9832 - val_loss: 0.6097 - val_acc: 0.9047\n",
      "Epoch 29/32\n",
      "157/157 [==============================] - 16s 102ms/step - loss: 0.0416 - acc: 0.9837 - val_loss: 0.7342 - val_acc: 0.8976\n",
      "Epoch 30/32\n",
      "157/157 [==============================] - 16s 102ms/step - loss: 0.0333 - acc: 0.9862 - val_loss: 0.8696 - val_acc: 0.8977\n",
      "Epoch 31/32\n",
      "157/157 [==============================] - 16s 102ms/step - loss: 0.0412 - acc: 0.9821 - val_loss: 0.6891 - val_acc: 0.9044\n",
      "Epoch 32/32\n",
      "157/157 [==============================] - 16s 102ms/step - loss: 0.0370 - acc: 0.9837 - val_loss: 0.8087 - val_acc: 0.8985\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs = 32,\n",
    "    validation_data = valid_generator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('invincea.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('invincea.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/157 [==============================] - 8s 51ms/step - loss: 0.8087 - acc: 0.8985\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.8086861371994019, 0.8985499739646912]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(valid_generator)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
